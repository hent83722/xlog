# Zyrnix v1.1.0 - New Features

## Overview

Version 1.1.0 introduces four major enterprise-grade features that enhance Zyrnix's capabilities for production environments:

1. **Rate Limiting & Sampling** - Prevent log flooding
2. **Compression Support** - Save 70-90% disk space
3. **Cloud Sinks** - Native AWS CloudWatch & Azure Monitor integration
4. **Metrics & Observability** - Built-in telemetry with Prometheus export

---

## 1. Rate Limiting & Sampling

### Problem Solved
During incidents or error storms, applications can flood logs, causing:
- Disk exhaustion
- Performance degradation
- Loss of critical information in noise

### Solution
Token bucket rate limiting and sampling to control log output intelligently.

### Features
- **Token Bucket Algorithm**: Allows bursts while maintaining rate limits
- **Per-Logger Limits**: Configure different limits for different loggers
- **Sampling**: Log every Nth message for high-frequency events
- **Combined Mode**: Use both rate limiting and sampling together
- **Statistics**: Track dropped message counts

### Quick Example
```cpp
#include <Zyrnix/rate_limiter.hpp>

// Rate limiter: 100 msg/sec, burst capacity 200
Zyrnix::RateLimiter limiter(100, 200);

for (int i = 0; i < 10000; ++i) {
    if (limiter.try_log()) {
        logger->error("Database error");
    }
}

std::cout << "Dropped: " << limiter.dropped_count() << "\n";
```

### API Reference

#### RateLimiter
```cpp
class RateLimiter {
public:
    RateLimiter(size_t messages_per_second, size_t burst_capacity);
    bool try_log();                  // Try to acquire token
    uint64_t dropped_count() const;  // Get dropped count
    void reset();                    // Reset state
};
```

#### SamplingLimiter
```cpp
class SamplingLimiter {
public:
    SamplingLimiter(size_t sample_rate); // 1 = all, 10 = 1 in 10
    bool should_log();
    uint64_t total_count() const;
    uint64_t dropped_count() const;
};
```

#### CombinedLimiter
```cpp
class CombinedLimiter {
public:
    CombinedLimiter(size_t messages_per_second, 
                    size_t burst_capacity,
                    size_t sample_rate);
    bool should_log();
    Stats get_stats() const;
};
```

---

## 2. Compression Support

### Problem Solved
Log files consume significant disk space, especially in high-throughput applications.

### Solution
Automatic compression of rotated log files using gzip or zstd.

### Features
- **Gzip Support**: Universal compatibility (1-9 compression levels)
- **Zstd Support**: Better compression ratios, faster (1-22 levels)
- **Compress-on-Rotate**: Automatic compression when rotating files
- **Configurable Levels**: Balance speed vs compression ratio
- **Statistics Tracking**: Monitor compression ratios and space saved
- **70-90% Space Savings**: Typical compression ratios

### Quick Example
```cpp
#include <Zyrnix/sinks/compressed_file_sink.hpp>

Zyrnix::CompressionOptions options;
options.type = Zyrnix::CompressionType::Gzip;
options.level = 6;                    // Default compression
options.compress_on_rotate = true;

auto sink = std::make_shared<Zyrnix::CompressedFileSink>(
    "app.log",
    10 * 1024 * 1024,  // 10 MB max file size
    30,                // Keep 30 rotated files
    options
);

auto logger = std::make_shared<Zyrnix::Logger>("app");
logger->add_sink(sink);

// Check compression stats
auto stats = sink->get_compression_stats();
std::cout << "Compression ratio: " << stats.compression_ratio << "x\n";
std::cout << "Space saved: " << ((stats.original_bytes - stats.compressed_bytes) * 100.0 / stats.original_bytes) << "%\n";
```

### API Reference

#### CompressionType
```cpp
enum class CompressionType {
    None,
    Gzip,  // Universal, 1-9 levels
    Zstd   // Modern, 1-22 levels
};
```

#### CompressedFileSink
```cpp
class CompressedFileSink : public LogSink {
public:
    CompressedFileSink(const std::string& filename,
                       size_t max_size,
                       size_t max_files,
                       const CompressionOptions& options);
    
    CompressionStats get_compression_stats() const;
};

struct CompressionStats {
    uint64_t files_compressed;
    uint64_t original_bytes;
    uint64_t compressed_bytes;
    double compression_ratio;
};
```

### Compression Level Guide

**Gzip Levels:**
- 1-3: Fast compression, lower ratio (~2-3x)
- 6: Balanced (default) (~3-4x)
- 9: Best compression, slower (~4-5x)

**Zstd Levels:**
- 1-3: Very fast, good ratio (~3-4x)
- 10-15: Balanced (~4-6x)
- 19-22: Maximum compression (~6-8x)

### Dependencies
```bash
# For gzip support
apt-get install zlib1g-dev

# For zstd support
apt-get install libzstd-dev
```

---

## 3. Cloud Sinks

### Problem Solved
Manual log shipping to cloud platforms is complex and error-prone.

### Solution
Native integration with AWS CloudWatch and Azure Monitor with batching, retry logic, and automatic failure handling.

### Features
- **AWS CloudWatch Logs**: Direct integration with CloudWatch
- **Azure Monitor / App Insights**: Native telemetry export
- **Batching**: Reduce API calls and costs
- **Retry Logic**: Exponential backoff for transient failures
- **Async Operation**: Non-blocking background threads
- **Statistics**: Monitor sent/failed/dropped messages
- **Queue Management**: Buffer spikes without blocking

### Quick Example - AWS CloudWatch
```cpp
#include <Zyrnix/sinks/cloud_sinks.hpp>

Zyrnix::CloudWatchSink::Config config;
config.region = "us-east-1";
config.log_group_name = "/aws/myapp";
config.log_stream_name = "instance-01";
config.batch_size = 100;           // Batch 100 messages
config.batch_timeout_ms = 5000;    // Or send after 5 seconds
config.max_retries = 3;            // Retry 3 times
config.max_queue_size = 10000;     // Buffer 10K messages

auto sink = std::make_shared<Zyrnix::CloudWatchSink>(config);
auto logger = std::make_shared<Zyrnix::Logger>("app");
logger->add_sink(sink);

logger->info("Logged to CloudWatch!");

// Check statistics
auto stats = sink->get_stats();
std::cout << "Sent: " << stats.messages_sent << "\n";
std::cout << "Failed: " << stats.messages_failed << "\n";
```

### Quick Example - Azure Monitor
```cpp
Zyrnix::AzureMonitorSink::Config config;
config.instrumentation_key = "your-key-here";
config.cloud_role_name = "my-service";
config.batch_size = 100;

auto sink = std::make_shared<Zyrnix::AzureMonitorSink>(config);
auto logger = std::make_shared<Zyrnix::Logger>("app");
logger->add_sink(sink);

logger->info("Logged to Azure Monitor!");
```

### API Reference

#### CloudWatchSink
```cpp
struct Config {
    std::string region;
    std::string log_group_name;
    std::string log_stream_name;
    std::string access_key_id;
    std::string secret_access_key;
    size_t batch_size = 100;
    size_t batch_timeout_ms = 5000;
    size_t max_retries = 3;
    size_t retry_delay_ms = 1000;
    size_t max_queue_size = 10000;
};

class CloudWatchSink : public LogSink {
public:
    CloudWatchSink(const Config& config);
    Stats get_stats() const;
};
```

#### AzureMonitorSink
```cpp
struct Config {
    std::string instrumentation_key;
    std::string ingestion_endpoint;
    std::string cloud_role_name;
    std::string cloud_role_instance;
    size_t batch_size = 100;
    size_t batch_timeout_ms = 5000;
    size_t max_retries = 3;
};

class AzureMonitorSink : public LogSink {
public:
    AzureMonitorSink(const Config& config);
    Stats get_stats() const;
};
```

### Cost Optimization Tips
- **Use aggressive batching** (500+ messages)
- **Apply rate limiting** to reduce volume
- **Use sampling** for debug/trace levels
- **Monitor costs** with statistics

**Pricing (approximate):**
- AWS CloudWatch: ~$0.50 per GB ingested
- Azure Monitor: ~$2.30 per GB ingested

### Dependencies
```bash
# libcurl for HTTP requests
apt-get install libcurl4-openssl-dev
```

---

## 4. Metrics & Observability

### Problem Solved
No visibility into logging system health, performance, or behavior.

### Solution
Built-in telemetry with Prometheus export format for monitoring logging infrastructure itself.

### Features
- **Performance Metrics**: Messages/sec, latency (avg, p99)
- **Health Metrics**: Dropped messages, errors, queue depth
- **Per-Logger Metrics**: Individual logger statistics
- **Per-Sink Metrics**: Sink-level performance tracking
- **Prometheus Export**: Standard format for Prometheus/Grafana
- **JSON Export**: REST API friendly format
- **Global Registry**: Centralized metrics management
- **Zero Overhead**: Atomic operations, minimal impact

### Quick Example
```cpp
#include <Zyrnix/log_metrics.hpp>

auto& registry = Zyrnix::MetricsRegistry::instance();
auto metrics = registry.get_logger_metrics("app");

// Metrics are automatically recorded during logging
logger->info("Message");

// Get snapshot
auto snapshot = metrics->get_snapshot();
std::cout << "Messages/sec: " << snapshot.messages_per_second << "\n";
std::cout << "Avg latency: " << snapshot.avg_log_latency_us << " Âµs\n";
std::cout << "Queue depth: " << snapshot.current_queue_depth << "\n";

// Export for Prometheus
std::string prom = registry.export_all_prometheus();
std::cout << prom;
```

### API Reference

#### LogMetrics
```cpp
class LogMetrics {
public:
    void record_message_logged();
    void record_message_dropped();
    void record_message_filtered();
    void record_flush();
    void record_log_duration(uint64_t microseconds);
    void record_flush_duration(uint64_t microseconds);
    void update_queue_depth(size_t depth);
    
    double get_messages_per_second() const;
    double get_average_log_latency_us() const;
    uint64_t get_max_log_latency_us() const;
    
    Snapshot get_snapshot() const;
    std::string export_prometheus(const std::string& prefix) const;
    std::string export_json() const;
};
```

#### MetricsRegistry
```cpp
class MetricsRegistry {
public:
    static MetricsRegistry& instance();
    
    std::shared_ptr<LogMetrics> get_logger_metrics(const std::string& name);
    std::shared_ptr<SinkMetrics> get_sink_metrics(const std::string& name);
    
    std::string export_all_prometheus(const std::string& prefix) const;
    std::string export_all_json() const;
    void reset_all();
};
```

### Prometheus Metrics Exported

```
Zyrnix_messages_logged_total          # Total messages logged
Zyrnix_messages_dropped_total         # Total messages dropped
Zyrnix_messages_filtered_total        # Total messages filtered
Zyrnix_messages_per_second           # Current logging rate
Zyrnix_log_latency_us_avg            # Average log latency
Zyrnix_log_latency_us_max            # Maximum log latency
Zyrnix_queue_depth                   # Current queue depth
Zyrnix_queue_depth_max               # Maximum queue depth
Zyrnix_errors_total                  # Total errors
Zyrnix_sink_writes_total{sink=""}    # Writes per sink
Zyrnix_sink_bytes_written_total{sink=""} # Bytes per sink
```

### Grafana Dashboard Ideas
- Log throughput over time
- Drop rate percentage
- P50/P95/P99 latency percentiles
- Queue depth heatmap
- Error rate by logger
- Bytes written by sink

---

## Build Configuration

### Feature Flags

All new features can be disabled to reduce binary size:

```bash
# Enable all features (default)
cmake -DXLOG_ENABLE_RATE_LIMITING=ON \
      -DXLOG_ENABLE_COMPRESSION=ON \
      -DXLOG_ENABLE_CLOUD_SINKS=ON \
      -DXLOG_ENABLE_METRICS=ON \
      ..

# Minimal build (disable all)
cmake -DXLOG_MINIMAL=ON ..

# Custom build (pick features)
cmake -DXLOG_ENABLE_RATE_LIMITING=ON \
      -DXLOG_ENABLE_COMPRESSION=OFF \
      -DXLOG_ENABLE_CLOUD_SINKS=OFF \
      -DXLOG_ENABLE_METRICS=ON \
      ..
```

### Optional Dependencies

```bash
# For compression support
apt-get install zlib1g-dev libzstd-dev

# For cloud sinks
apt-get install libcurl4-openssl-dev

# Check what's available
./Zyrnix_check_features
```

---

## Examples

Complete working examples are in the `examples/` directory:

- `rate_limiting_example.cpp` - Rate limiting and sampling
- `compression_example.cpp` - Log file compression
- `cloud_sinks_example.cpp` - AWS and Azure integration
- `metrics_example.cpp` - Metrics and observability

Build examples:
```bash
cd examples
g++ -std=c++17 rate_limiting_example.cpp -lZyrnix -o rate_limiting_example
./rate_limiting_example
```

---

## Migration Guide

### From v1.0.4 to v1.1.0

**All existing code continues to work** - these are additive features.

### Adding Rate Limiting
```cpp
// Before (v1.0.4)
logger->error("Error message");

// After (v1.1.0) - with rate limiting
Zyrnix::RateLimiter limiter(100, 200);
if (limiter.try_log()) {
    logger->error("Error message");
}
```

### Switching to Compressed Sink
```cpp
// Before (v1.0.4)
auto sink = std::make_shared<Zyrnix::RotatingFileSink>("app.log", max_size, max_files);

// After (v1.1.0) - with compression
Zyrnix::CompressionOptions opts;
opts.type = Zyrnix::CompressionType::Gzip;
auto sink = std::make_shared<Zyrnix::CompressedFileSink>("app.log", max_size, max_files, opts);
```

### Adding Metrics
```cpp
// Metrics are automatic - just query them
auto& registry = Zyrnix::MetricsRegistry::instance();
auto metrics = registry.get_logger_metrics("app");
auto snapshot = metrics->get_snapshot();
```

---

## Performance Impact

### Rate Limiting
- **Overhead**: ~50-100ns per try_log() call
- **Memory**: ~64 bytes per limiter

### Compression
- **CPU**: Depends on level (1-3: minimal, 9: significant)
- **Memory**: ~256KB buffer per sink
- **I/O**: Reduced by 70-90%

### Cloud Sinks
- **Overhead**: Near zero (async batching)
- **Memory**: Configurable queue size
- **Network**: Efficient batching reduces API calls

### Metrics
- **Overhead**: ~10-20ns per metric record (atomic operations)
- **Memory**: ~512 bytes per logger

---

## Best Practices

### Rate Limiting
1. Start with burst capacity = 2x rate limit
2. Monitor dropped counts in metrics
3. Use sampling for debug/trace levels
4. Different limits for different loggers

### Compression
1. Level 3 for high-throughput apps
2. Level 6 (default) for most cases
3. Level 9 for archival storage
4. Zstd for best balance of speed/ratio

### Cloud Sinks
1. Batch aggressively (500+ messages)
2. Set large queue sizes for traffic spikes
3. Monitor statistics for health checks
4. Use rate limiting to control costs

### Metrics
1. Expose /metrics endpoint for Prometheus
2. Alert on high drop rates (> 1%)
3. Monitor queue depth for backpressure
4. Track P99 latency, not just average

---

## Troubleshooting

### Compression not working?
```bash
# Check dependencies
apt-get install zlib1g-dev libzstd-dev
cmake .. -DXLOG_ENABLE_COMPRESSION=ON
```

### Cloud sinks failing?
```bash
# Check libcurl
apt-get install libcurl4-openssl-dev
# Verify network connectivity
curl -X POST https://logs.us-east-1.amazonaws.com/
```

### High dropped message count?
- Increase rate limit
- Increase queue size
- Add more aggressive sampling
- Check sink performance

### High latency?
- Use async logging
- Increase flush interval
- Check disk I/O
- Monitor metrics for bottlenecks

---

## Roadmap for v1.2.0

Potential future enhancements:
- Kafka sink
- OpenTelemetry integration
- Memory-mapped file sink
- SIMD-accelerated formatting
- Python bindings

---

## Contributing

We welcome contributions! Areas of interest:
- Additional cloud sinks (GCP, Datadog, etc.)
- Compression algorithm optimization
- More metrics and observability features
- Performance improvements

---

## License

MIT License - same as Zyrnix core

---

## Support

- GitHub Issues: https://github.com/hent83722/Zyrnix/issues
- Documentation: https://github.com/hent83722/Zyrnix/docs
- Examples: https://github.com/hent83722/Zyrnix/examples
